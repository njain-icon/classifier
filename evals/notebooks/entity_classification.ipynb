{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b61c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entity classification mapping dictionary.\n",
    "\n",
    "This dictionary maps entity types to their corresponding identifiers\n",
    "used in the entity classification system.\n",
    "\"\"\"\n",
    "\n",
    "mapping_dict = {\n",
    "#\"LLM_PERSON\": \"person-name\",\n",
    "#\"STREET_ADDRESS\": \"street-address\",\n",
    "\"DATE_OF_BIRTH\": \"date-of-birth\",\n",
    "\"US_SSN\": \"us-ssn\",\n",
    "\"US_DRIVER_LICENSE\": \"us-drivers-license\",\n",
    "\"PHONE_NUMBER\": \"phone-number\",\n",
    "\"EMAIL_ADDRESS\": \"email-address\",\n",
    "\"US_BANK_NUMBER\": \"us-bank-account-number\",\n",
    "\"ROUTING_NUMBER\": \"bank-routing-number\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e697a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Tuple, List, Dict, Any, Set\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_classification_api(text, mode=\"all\", anonymize=False):\n",
    "    \"\"\"\n",
    "    Call the Pebblo classification API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify\n",
    "        mode (str): Classification mode - \"all\", \"entity\", or \"topic\"\n",
    "        anonymize (bool): Whether to anonymize the results\n",
    "    \n",
    "    Returns:\n",
    "        dict: The classification response\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:8000/api/v1/classify\"\n",
    "    \n",
    "    # Prepare the request payload without llm_config\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"anonymize\": anonymize,\n",
    "        \"country_list\": [\"US\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #print(\"payload\", payload)\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making API request: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_entity_test_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the entity test data from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the test data.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "        ValueError: If the file cannot be read as an Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError as fnf_err:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        raise fnf_err\n",
    "    except Exception as exc:\n",
    "        print(f\"Error reading Excel file: {exc}\")\n",
    "        raise ValueError(f\"Could not read Excel file: {file_path}\") from exc\n",
    "\n",
    "# Example usage:\n",
    "# test_data_df = read_entity_test_data(\"entity_test_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce54264",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excel=\"../test_data/final_entity_dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_df = read_entity_test_data(input_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874221bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"Experiment_13\"\n",
    "description = \"\"\n",
    "tags = []\n",
    "test_column = \"mapped_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f840a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_api_response_to_entity_list(api_response: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Converts an API response with 'entityDetails' and 'data' fields into a list of entity dicts\n",
    "    in the format: {'start': int, 'end': int, 'label': str, 'extracted_text': str}.\n",
    "\n",
    "    Args:\n",
    "        api_response (Dict[str, Any]): The API response containing 'entityDetails' and 'data'.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of entities in the required format.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If required keys are missing in the API response.\n",
    "        ValueError: If location format is invalid.\n",
    "    \"\"\"\n",
    "    entities: List[Dict[str, Any]] = []\n",
    "    entity_details = api_response.get(\"entityDetails\", {})\n",
    "    text_data = api_response.get(\"data\", \"\")\n",
    "\n",
    "    if not entity_details or not isinstance(entity_details, dict):\n",
    "        return entities\n",
    "\n",
    "    for label, entity_list in entity_details.items():\n",
    "        for entity in entity_list:\n",
    "            location = entity.get(\"location\")\n",
    "            if not location or \"_\" not in location:\n",
    "                continue\n",
    "            try:\n",
    "                start_str, end_str = location.split(\"_\")\n",
    "                start = int(start_str)\n",
    "                end = int(end_str)\n",
    "                extracted_text = text_data[start:end]\n",
    "                entities.append({\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"label\": label,\n",
    "                    \"extracted_text\": extracted_text\n",
    "                })\n",
    "            except (ValueError, TypeError) as exc:\n",
    "                print(f\"Skipping entity with invalid location '{location}': {exc}\")\n",
    "                continue\n",
    "    return entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_classification_results(\n",
    "    df: pd.DataFrame, text_column: str = \"text\", max_workers: int = 6\n",
    ") -> (List[Dict[str, Any]], List[float]):\n",
    "    \"\"\"\n",
    "    Iterates over each row in the DataFrame, sends the specified text column to the classification API in parallel,\n",
    "    and aggregates the results in a list.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the test data.\n",
    "        text_column (str): Name of the column containing the text to classify.\n",
    "        max_workers (int): Maximum number of threads to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[float]]: List of classification results and response times for each row.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified text_column does not exist in the DataFrame.\n",
    "    \"\"\"\n",
    "    if text_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "    results: List[Any] = [None] * len(df)\n",
    "    response_time: List[float] = [0.0] * len(df)\n",
    "\n",
    "    def process_row(idx: int, text: str):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing row {idx} of {len(df)}\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = call_classification_api(text, mode=\"entity\")\n",
    "            converted = convert_api_response_to_entity_list(result)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing row {idx}: {exc}\")\n",
    "            converted = {\"row_index\": idx, \"error\": str(exc)}\n",
    "        end_time = time.time()\n",
    "        return idx, converted, end_time - start_time\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(process_row, idx, row.get(text_column, \"\")): idx\n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx, converted, elapsed = future.result()\n",
    "            results[idx] = converted\n",
    "            response_time[idx] = elapsed\n",
    "\n",
    "    return results, response_time\n",
    "\n",
    "\n",
    "# Aggregate results for the first 10 rows\n",
    "start_time = time.time()\n",
    "classification_results, response_time = aggregate_classification_results(\n",
    "    test_data_df, text_column=\"text\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dba870",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df[f\"response_time_{experiment_name}\"] = response_time\n",
    "test_data_df[f\"classification_results_{experiment_name}\"] = classification_results\n",
    "test_data_df.to_excel(input_excel, index=False)\n",
    "experiment_time = time.time() - start_time\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c044bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_pairs_from_list(entity_list: List[dict]) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts (entity_type, value) pairs from a list of entity dicts.\n",
    "\n",
    "    Args:\n",
    "        entity_list (List[dict]): List of entity dicts, each with at least 'label' and 'extracted_text' keys.\n",
    "\n",
    "    Returns:\n",
    "        Set[Tuple[str, str]]: Set of (entity_type, value) pairs.\n",
    "    \"\"\"\n",
    "    entity_pairs = set()\n",
    "    if not isinstance(entity_list, list):\n",
    "        return entity_pairs\n",
    "    for ent in entity_list:\n",
    "        ent_type = ent.get(\"label\")\n",
    "        value = ent.get(\"extracted_text\").strip()\n",
    "        if ent_type is not None and value is not None:\n",
    "            entity_pairs.add((ent_type, str(value)))\n",
    "    return entity_pairs\n",
    "\n",
    "def compute_entity_metrics_for_lists(\n",
    "    df: pd.DataFrame,\n",
    "    actual_col: str = \"mapped_output\",\n",
    "    pred_col: str = None,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Computes per-entity and overall metrics (accuracy, precision, recall, F1) for entity classification,\n",
    "    assuming both actual and predicted entities are lists of dicts.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with ground truth in `actual_col`.\n",
    "        classification_results (List[List[dict]]): List of predicted entity lists (one per row).\n",
    "        actual_col (str): Name of the column with actual entity lists.\n",
    "        pred_col (str, optional): If not None, use this column in df for predicted entity lists.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, float]]: (Per-entity metrics DataFrame, overall metrics dict)\n",
    "    \"\"\"\n",
    "    entity_stats = defaultdict(lambda: {\n",
    "        \"support\": 0,\n",
    "        \"actual_count\": 0,\n",
    "        \"correct\": 0,\n",
    "        \"extra\": 0,\n",
    "        \"missed\": 0,\n",
    "        \"wrong\": 0,\n",
    "        \"tp\": 0,\n",
    "        \"fp\": 0,\n",
    "        \"fn\": 0,\n",
    "    })\n",
    "\n",
    "    all_entity_types = set()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get actual entities as list of dicts\n",
    "        actual_entities = row[actual_col]\n",
    "        # INSERT_YOUR_CODE\n",
    "        # Replace any curly single quotes (‘ or ’) with straight single quotes (')\n",
    "        if isinstance(actual_entities, str):\n",
    "            actual_entities = actual_entities.replace('‘', \"'\").replace('’', \"'\")\n",
    "            #actual_entities = actual_entities.replace('’', \"'\")\n",
    "            actual_entities = actual_entities.replace('‘', '\"')\n",
    "        pred_entities = row[pred_col]\n",
    "\n",
    "        if isinstance(actual_entities, str):\n",
    "            try:\n",
    "                actual_entities = eval(actual_entities)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in actual_entities\", e)\n",
    "                actual_entities = []\n",
    "        if not isinstance(actual_entities, list):\n",
    "            actual_entities = []\n",
    "        if isinstance(pred_entities, str):\n",
    "            try:\n",
    "                pred_entities = eval(pred_entities)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in pred_entities\", e)\n",
    "                pred_entities = []\n",
    "        if not isinstance(pred_entities, list):\n",
    "            pred_entities = []\n",
    "        # Build sets of (entity_type, value) for actual and predicted\n",
    "\n",
    "        actual_set = extract_entity_pairs_from_list(actual_entities)\n",
    "        pred_set = extract_entity_pairs_from_list(pred_entities)\n",
    "\n",
    "\n",
    "        actual_types = set([et for et, _ in actual_set])\n",
    "        pred_types = set([et for et, _ in pred_set])\n",
    "        all_types = actual_types | pred_types\n",
    "        all_entity_types.update(all_types)\n",
    "\n",
    "        # Update support and actual_count\n",
    "        for ent_type in actual_types:\n",
    "            entity_stats[ent_type][\"support\"] += 1\n",
    "            entity_stats[ent_type][\"actual_count\"] += sum(1 for t, _ in actual_set if t == ent_type)\n",
    "\n",
    "        # For each entity type, update stats\n",
    "        for ent_type in all_types:\n",
    "            actual_vals = set([v for t, v in actual_set if t == ent_type])\n",
    "            pred_vals = set([v for t, v in pred_set if t == ent_type])\n",
    "            correct = actual_vals & pred_vals\n",
    "            extra = pred_vals - actual_vals\n",
    "            missed = actual_vals - pred_vals\n",
    "            # \"wrong\" is not well-defined for this case, but we keep it for compatibility\n",
    "            wrong = set()\n",
    "\n",
    "            entity_stats[ent_type][\"correct\"] += len(correct)\n",
    "            entity_stats[ent_type][\"extra\"] += len(extra)\n",
    "            entity_stats[ent_type][\"missed\"] += len(missed)\n",
    "            entity_stats[ent_type][\"wrong\"] += len(wrong)\n",
    "            entity_stats[ent_type][\"tp\"] += len(correct)\n",
    "            entity_stats[ent_type][\"fp\"] += len(extra)\n",
    "            entity_stats[ent_type][\"fn\"] += len(missed)\n",
    "\n",
    "    # Compute metrics per entity\n",
    "    metrics = []\n",
    "    for ent_type in sorted(all_entity_types):\n",
    "        stats = entity_stats[ent_type]\n",
    "        tp = stats[\"tp\"]\n",
    "        fp = stats[\"fp\"]\n",
    "        fn = stats[\"fn\"]\n",
    "        support = stats[\"support\"]\n",
    "        actual_count = stats[\"actual_count\"]\n",
    "        denom = tp + fp + fn\n",
    "        accuracy = tp / denom if denom > 0 else 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        metrics.append({\n",
    "            \"entity_type\": ent_type,\n",
    "            \"actual_count\": actual_count,\n",
    "            \"support\": support,\n",
    "            \"correct\": stats[\"correct\"],\n",
    "            \"extra\": stats[\"extra\"],\n",
    "            \"missed\": stats[\"missed\"],\n",
    "            \"wrong\": stats[\"wrong\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Compute macro averages (across all entities)\n",
    "    macro = {}\n",
    "    for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "        macro[f\"macro_{metric}\"] = metrics_df[metric].mean() if not metrics_df.empty else 0.0\n",
    "\n",
    "    return metrics_df, macro\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c540a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assume test_data_df[\"mapped_output\"] and classification_results are both lists of dicts per row\n",
    "entity_metrics_df, macro_metrics = compute_entity_metrics_for_lists(\n",
    "    test_data_df, actual_col=test_column, pred_col=f\"classification_results_{experiment_name}\"\n",
    ")\n",
    "# Sort the entity_metrics_df by f1 score in descending order for better interpretability\n",
    "entity_metrics_df = entity_metrics_df.sort_values(by=\"f1\", ascending=False).reset_index(drop=True)\n",
    "entity_metrics_df = entity_metrics_df[entity_metrics_df[\"f1\"] > 0].reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "display(entity_metrics_df[[\"entity_type\", \"actual_count\", \"correct\", \"extra\", \"missed\", \"wrong\", \"accuracy\", \"precision\", \"recall\", \"f1\"]])\n",
    "print(\"Macro-averaged metrics across all entities:\")\n",
    "for k, v in macro_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73212756",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in the test_data_df DataFrame\n",
    "nan_count = test_data_df.isna().sum().sum()\n",
    "print(f\"Total number of NaN values in test_data_df: {nan_count}\")\n",
    "# Identify columns in test_data_df that contain NaN values and print them\n",
    "\n",
    "def print_columns_with_nan(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints the names of columns in the DataFrame that contain NaN values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check for NaN values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    nan_columns = df.columns[df.isna().any()].tolist()\n",
    "    if nan_columns:\n",
    "        print(f\"Columns with NaN values: {nan_columns}\")\n",
    "    else:\n",
    "        print(\"No columns contain NaN values.\")\n",
    "\n",
    "num_nan_classification_results = test_data_df[\"classification_results\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'classification_results': {num_nan_classification_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75184544",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.iloc[1395]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rows_with_extra_entity(\n",
    "    test_data_df: pd.DataFrame,\n",
    "    entity_label: str,\n",
    "    actual_col: str = \"mapped_output\",\n",
    "    pred_col: str = \"classification_results\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows from test_data_df where the specified entity_label was detected as extra\n",
    "    in classification_results (i.e., present in prediction but not in mapped_output).\n",
    "\n",
    "    Only the specified entity_label is checked. The entity_label should match the label\n",
    "    used in the entity extraction output (e.g., 'bank-account-number', 'ssn', etc.).\n",
    "\n",
    "    Args:\n",
    "        test_data_df (pd.DataFrame): The original test data DataFrame.\n",
    "        classification_results (list[dict]): List of dicts with extracted entities per row.\n",
    "        entity_label (str): The entity type to check for extra detections.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Subset of test_data_df where entity_label was extra.\n",
    "    \"\"\"\n",
    "    extra_indices = []\n",
    "    for idx, (actual, predicted) in enumerate(\n",
    "        zip(test_data_df[actual_col], test_data_df[pred_col])\n",
    "    ):\n",
    "        # Ensure 'actual' is a list of dicts, not a string\n",
    "        if isinstance(actual, str):\n",
    "            try:\n",
    "                import ast\n",
    "                actual = actual.replace('‘', \"'\").replace('’', \"'\")\n",
    "            #actual_entities = actual_entities.replace('’', \"'\")\n",
    "                actual = actual.replace('‘', '\"')\n",
    "                actual = ast.literal_eval(actual)\n",
    "            except (ValueError, SyntaxError):\n",
    "                actual = []\n",
    "        # Only consider entities of the specified label\n",
    "        actual_entities = [\n",
    "            ent for ent in actual if ent.get(\"label\") == entity_label\n",
    "        ]\n",
    "        if isinstance(predicted, str):\n",
    "            try:\n",
    "                import ast\n",
    "                predicted = ast.literal_eval(predicted)\n",
    "                print(\"predicted\",predicted)\n",
    "            except (ValueError, SyntaxError):\n",
    "                predicted_entities = []\n",
    "\n",
    "        try:    \n",
    "            predicted_entities = [\n",
    "                ent for ent in predicted if ent.get(\"label\") == entity_label\n",
    "            ]  \n",
    "        except:\n",
    "            print(\"predicted - idx\",idx, predicted)\n",
    "            predicted_entities = []\n",
    "\n",
    "        # If there are predicted entities but none actual, mark as extra\n",
    "        if predicted_entities and not actual_entities:\n",
    "            extra_indices.append(idx)\n",
    "        else:\n",
    "            # Check if any predicted entity is not matched by actual (by extracted_text or span)\n",
    "            for pred_ent in predicted_entities:\n",
    "                match_found = any(\n",
    "                    (pred_ent.get(\"extracted_text\") == act_ent.get(\"extracted_text\"))\n",
    "                    or (\n",
    "                        pred_ent.get(\"start\") == act_ent.get(\"start\")\n",
    "                        and pred_ent.get(\"end\") == act_ent.get(\"end\")\n",
    "                    )\n",
    "                    for act_ent in actual_entities\n",
    "                )\n",
    "                if not match_found:\n",
    "                    extra_indices.append(idx)\n",
    "                    break  # Only need to add the row once\n",
    "\n",
    "    return test_data_df.iloc[extra_indices]\n",
    "\n",
    "# Example usage:\n",
    "# To get rows with extra detections for a specific entity type, e.g., \"bank-account-number\" or \"ssn\":\n",
    "# extra_bank_account_rows = get_rows_with_extra_entity(test_data_df, classification_results, \"bank-account-number\")\n",
    "extra_entity = get_rows_with_extra_entity(test_data_df, \"uk-sort-code\", \"mapped_output\", f\"classification_results_{experiment_name}\")\n",
    "len(extra_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"classification_results_{experiment_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import ast\n",
    "\n",
    "def extract_entities_from_results(\n",
    "    rows_df,\n",
    "    classification_results: List[List[Dict[str, Any]]],\n",
    "    entity_label: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Extracts the specified entity from classification_results for the given rows.\n",
    "\n",
    "    Args:\n",
    "        rows_df (pd.DataFrame): DataFrame containing the rows of interest (e.g., extra_entity).\n",
    "        classification_results (List[List[Dict[str, Any]]]): List of entity dicts per row.\n",
    "        entity_label (str): The entity type to extract.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of extracted entity texts for the specified entity_label.\n",
    "    \"\"\"\n",
    "    extracted_entities = []\n",
    "    for idx in rows_df.index:\n",
    "        # Defensive: classification_results may be a list of dicts or a string\n",
    "        row_results = classification_results[idx]\n",
    "        if isinstance(row_results, str):\n",
    "            try:\n",
    "                row_results = ast.literal_eval(row_results)\n",
    "            except (ValueError, SyntaxError):\n",
    "                row_results = []\n",
    "        for ent in row_results:\n",
    "            if ent.get(\"label\") == entity_label and \"extracted_text\" in ent:\n",
    "                extracted_entities.append(ent[\"extracted_text\"])\n",
    "    return extracted_entities\n",
    "\n",
    "# Example usage:\n",
    "# Extract all phone numbers from extra_entity rows\n",
    "phone_numbers = extract_entities_from_results(extra_entity, classification_results, \"phone-number\")\n",
    "print(\"Extracted phone numbers:\", phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_numbers.index('284-38-7491')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ddc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows where 'bank-routing-number' was missed\n",
    "def get_rows_with_missed_entity(\n",
    "    test_data_df: pd.DataFrame,\n",
    "    classification_results: list[dict],\n",
    "    entity_label: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows from test_data_df where the specified entity_label was missed in classification_results.\n",
    "\n",
    "    Args:\n",
    "        test_data_df (pd.DataFrame): The original test data DataFrame.\n",
    "        classification_results (list[dict]): List of dicts with extracted entities per row.\n",
    "        entity_label (str): The entity type to check for missed detections.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Subset of test_data_df where entity_label was missed.\n",
    "    \"\"\"\n",
    "    missed_indices = []\n",
    "    for idx, (actual, predicted) in enumerate(\n",
    "        zip(test_data_df[\"mapped_output\"], classification_results)\n",
    "    ):\n",
    "        # Ensure 'actual' is a list of dicts, not a string\n",
    "        if isinstance(actual, str):\n",
    "            try:\n",
    "                import ast\n",
    "                actual = ast.literal_eval(actual)\n",
    "            except (ValueError, SyntaxError):\n",
    "                actual = []\n",
    "        # Get all actual entities of the target type\n",
    "        actual_entities = [\n",
    "            ent for ent in actual if ent.get(\"label\") == entity_label\n",
    "        ]\n",
    "        # Get all predicted entities of the target type\n",
    "        predicted_entities = [\n",
    "            ent for ent in predicted if ent.get(\"label\") == entity_label\n",
    "        ]\n",
    "        # If there are actual entities but none predicted, mark as missed\n",
    "        if actual_entities and not predicted_entities:\n",
    "            missed_indices.append(idx)\n",
    "        else:\n",
    "            # Check if any actual entity is not matched by prediction (by extracted_text or span)\n",
    "            for act_ent in actual_entities:\n",
    "                match_found = any(\n",
    "                    (act_ent.get(\"extracted_text\") == pred_ent.get(\"extracted_text\"))\n",
    "                    or (\n",
    "                        act_ent.get(\"start\") == pred_ent.get(\"start\")\n",
    "                        and act_ent.get(\"end\") == pred_ent.get(\"end\")\n",
    "                    )\n",
    "                    for pred_ent in predicted_entities\n",
    "                )\n",
    "                if not match_found:\n",
    "                    missed_indices.append(idx)\n",
    "                    break\n",
    "\n",
    "    return test_data_df.iloc[missed_indices]\n",
    "\n",
    "# Usage: get all rows where 'bank-routing-number' was missed\n",
    "missed_rows = get_rows_with_missed_entity(\n",
    "    test_data_df, classification_results, entity_label=\"uk-nino\",\n",
    ")\n",
    "\n",
    "len(missed_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all bank account numbers from the missed rows\n",
    "def extract_bank_account_numbers(df, missed_label) -> list[str]:\n",
    "    \"\"\"Extracts all unique bank account numbers from the 'mapped_output' column of the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'mapped_output' column with entity extraction results.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of unique bank account numbers found in the missed rows.\n",
    "    \"\"\"\n",
    "    bank_account_numbers = set()\n",
    "    for entities in df[\"mapped_output\"]:\n",
    "        # Ensure entities is a list of dicts\n",
    "        \n",
    "        if isinstance(entities, str):\n",
    "            try:\n",
    "                import ast\n",
    "                entities = ast.literal_eval(entities)\n",
    "            except (ValueError, SyntaxError):\n",
    "                entities = []\n",
    "        for ent in entities:\n",
    "            if (\n",
    "                isinstance(ent, dict)\n",
    "                and ent.get(\"label\") == missed_label\n",
    "                and ent.get(\"extracted_text\")\n",
    "            ):\n",
    "                bank_account_numbers.add(ent[\"extracted_text\"])\n",
    "    return list(bank_account_numbers)\n",
    "\n",
    "missed_bank_account_numbers = extract_bank_account_numbers(missed_rows, \"phone-number\")\n",
    "print(\"Missed :\")\n",
    "for acc_num in missed_bank_account_numbers:\n",
    "    print(acc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcac3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=2\n",
    "text = extra_entity.iloc[index][\"text\"]\n",
    "op = call_classification_api(text)\n",
    "new_op =convert_api_response_to_entity_list(op)\n",
    "new_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_entity.iloc[index][\"mapped_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_entity.iloc[index][f\"classification_results_{experiment_name}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_entity.to_excel(\"extra_entity.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"   APPLICATION FOR HOUSING BENEFIT - Section C: Personal Details. Please complete all mandatory fields marked with an asterisk (*). Full name*: Marcus Johnson. Date of birth*: 15/03/1987. National Insurance number*: TK571394B. NHS number (if known): 785 341 9672. Current residential address*: 67 Elm Grove, Leeds, LS2 9JT. Are you the tenant? YES. Contact telephone number*: +44 113 542 8671. Email address: m.johnson@email.co.uk. Bank details for payments: Sort code: 40-52-71, Account number: 63847295, Account holder: Marcus Johnson. Declaration: I certify that the information provided is true and complete. Any false statements may result in prosecution. For office use only - Reference: HB/2025/3847. Processing officer: Sarah Mitchell, Leeds City Council.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc541799",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_classification_api(text, mode=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[1324:1331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_license_plate_number_in_excel(\n",
    "    excel_path: str,\n",
    "    input_file_path: str,\n",
    "    experiment_col: str = \"classification_results_Experiment_5\",\n",
    "    mapped_col: str = \"mapped_output\",\n",
    "    index_col: str = \"index\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Updates the mapped_output column in the Excel file for the given index,\n",
    "    setting the mapped_output to only the license_plate_number entity/entities\n",
    "    found in the classification_results_Experiment_5 column.\n",
    "\n",
    "    Args:\n",
    "        excel_path (str): Path to the extra_entity Excel file.\n",
    "        input_file_path (str): Path to the input file (not used for writing, just for context).\n",
    "        experiment_col (str): Name of the column with classification results.\n",
    "        mapped_col (str): Name of the column to update.\n",
    "        index_col (str): Name of the index column.\n",
    "    \"\"\"\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Ensure the experiment column exists\n",
    "    if experiment_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{experiment_col}' not found in Excel file.\")\n",
    "\n",
    "    # Ensure the mapped column exists\n",
    "    if mapped_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{mapped_col}' not found in Excel file.\")\n",
    "\n",
    "    # Ensure the index column exists\n",
    "    if index_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{index_col}' not found in Excel file.\")\n",
    "\n",
    "    # Iterate over each row and update mapped_output for license_plate_number\n",
    "    for idx, row in df.iterrows():\n",
    "        classification_results = row[experiment_col]\n",
    "        # Defensive: handle stringified lists/dicts\n",
    "        if isinstance(classification_results, str):\n",
    "            try:\n",
    "                import ast\n",
    "                classification_results = ast.literal_eval(classification_results)\n",
    "            except Exception:\n",
    "                continue  # skip if cannot parse\n",
    "\n",
    "        if not isinstance(classification_results, list):\n",
    "            continue\n",
    "\n",
    "        # Extract only license_plate_number entities\n",
    "        license_plate_entities = [\n",
    "            entity for entity in classification_results\n",
    "            if isinstance(entity, dict) and entity.get(\"label\", \"\").lower() == \"license_plate_number\"\n",
    "        ]\n",
    "\n",
    "        # Update the mapped_output column for this row\n",
    "        df.at[idx, mapped_col] = str(license_plate_entities)\n",
    "\n",
    "    # Save the updated DataFrame back to Excel\n",
    "    df.to_excel(excel_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "# update_license_plate_number_in_excel(\"extra_entity.xlsx\", \"input_file.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84890e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"/Users/nishanjain/Downloads/german.txt\")\n",
    "txt = f.read()\n",
    "op = json.loads(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/Users/nishanjain/Downloads/german_2.txt\")\n",
    "txt = f.read()\n",
    "op =  op + json.loads(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e84e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = []\n",
    "op_list = []\n",
    "for o in op:\n",
    "    txt_list.append(o[\"Text\"])\n",
    "    op_list.append(o[\"Entities\"])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"text\": txt_list, \"mapped_output\": op_list})\n",
    "df.to_excel(\"german.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f00fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df and test_data_df, keeping only 'text' and 'mapped_output' columns\n",
    "combined_df = pd.concat([\n",
    "    df[[\"text\", \"mapped_output\"]],\n",
    "    test_data_df[[\"text\", \"mapped_output\"]]\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9bd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results, response_time = aggregate_classification_results(df, text_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54103bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f\"response_time_{experiment_name}\"] = response_time\n",
    "df[f\"classification_results_{experiment_name}\"] = classification_results\n",
    "#test_data_df.to_excel(input_excel, index=False)\n",
    "experiment_time = time.time() - start_time\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d499caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assume test_data_df[\"mapped_output\"] and classification_results are both lists of dicts per row\n",
    "entity_metrics_df, macro_metrics = compute_entity_metrics_for_lists(\n",
    "    df, actual_col=test_column, pred_col=f\"classification_results_{experiment_name}\"\n",
    ")\n",
    "# Sort the entity_metrics_df by f1 score in descending order for better interpretability\n",
    "entity_metrics_df = entity_metrics_df.sort_values(by=\"f1\", ascending=False).reset_index(drop=True)\n",
    "#entity_metrics_df = entity_metrics_df[entity_metrics_df[\"f1\"] > 0].reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "display(entity_metrics_df[[\"entity_type\", \"actual_count\", \"correct\", \"extra\", \"missed\", \"wrong\", \"accuracy\", \"precision\", \"recall\", \"f1\"]])\n",
    "print(\"Macro-averaged metrics across all entities:\")\n",
    "for k, v in macro_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_entity = get_rows_with_extra_entity(df, \"german-tax-identification-number\", \"mapped_output\", f\"classification_results_{experiment_name}\")\n",
    "len(extra_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "def call_classification_api(text, mode=\"all\", anonymize=False):\n",
    "    \"\"\"\n",
    "    Call the Pebblo classification API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify\n",
    "        mode (str): Classification mode - \"all\", \"entity\", or \"topic\"\n",
    "        anonymize (bool): Whether to anonymize the results\n",
    "    \n",
    "    Returns:\n",
    "        dict: The classification response\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL.rstrip('/')}/api/v1/classify\"\n",
    "    \n",
    "    # Prepare the request payload without llm_config\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"anonymize\": anonymize,\n",
    "        \"country_list\": [\"US\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #print(\"payload\", payload)\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making API request: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_rows = get_rows_with_missed_entity(df, \n",
    "                                        classification_results, \n",
    "                                        entity_label=\"german-identity-card-number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "RMEDICAL CERTIFICATE - Dr. med. Petra Hoffmann, Specialist in Internal Medicine. Practice: North Health Center, Hamburger Straße 89, 20459 Hamburg, Tel: +49 40 3847 2956. PATIENT DETAILS: Mrs. Maria Schneider, born 23.04.1978, residing at: Rosenstraße 34, 22301 Hamburg, Tel: +49 40 8394 5627. Identity Card: L32RX8H29D, Health Insurance: AOK Hamburg, Insurance Number: M847392654. DIAGNOSIS: Acute bronchitis (ICD-10: J20.9), first occurred on 18.10.2025. TREATMENT: Antibiotic therapy with Amoxicillin 500mg, three times daily for 7 days. Medical leave from 19.10.2025 to 26.10.2025 (8 calendar days). FOLLOW-UP: Control examination scheduled for 28.10.2025 at 2:30 PM. Return immediately if symptoms worsen. Doctor's tax details: Tax ID: 41 738 29 465 6, VAT ID: DE394728565. Patient's driver's license: H72M3N8V39C (temporary driving restriction due to medication). Hamburg, October 19, 2025. Dr. med. Petra Hoffmann, Physician.\"\n",
    " **Medical Loan Application**\n",
    "\n",
    "Full Name: Reiner P. Misicher\n",
    "Date of Birth: DD/MM/YYYY\n",
    "National Insurance Number: MLNSPH31D16F536O\n",
    "Contact Telephone Number: [+44] 1234567890\n",
    "Email Address: r.misicher@example.com\n",
    "\n",
    "Residential Address: 89, rue Auguste Lelièvre, London, N1 9AB, United Kingdom\n",
    "\n",
    "Employment Information:\n",
    "Employer's Name: St. Bartholomew's Hospital\n",
    "Job Title: Consultant Cardiologist\n",
    "Annual Income: £90,000\n",
    "\n",
    "Medical History:\n",
    "1. Chronic Condition: Hypertension - Diagnosed in 2010\n",
    "2. Current Medications: Ramipril 5mg, Amlodipine 10mg\n",
    "3. Allergies: No known allergies\n",
    "\n",
    "Loan Information:\n",
    "Loan Amount Requested: £20,000\n",
    "Loan Purpose: Heart Valve Replacement Surgery\n",
    "Estimated Treatment Cost: £25,000\n",
    "Hospital: King's College Hospital, London\n",
    "Tentative Surgery Date: 01/10/2023\n",
    "Referring Physician: Dr. Sarah K. Johnson, Consultant Cardiothoracic Surgeon\n",
    "\n",
    "Personal Declaration:\n",
    "I, Reiner P. Misicher, hereby declare that all the information provided in this loan application is true and accurate to the best of my knowledge. I understand that any misrepresentation of facts may lead to the cancellation of the loan or legal consequences.\n",
    "\n",
    "Signature: Reiner P. Misicher\n",
    "Date: DD/MM/YYYY\n",
    "Email Address: r.misicher@example.com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "#text = missed_rows.iloc[index][\"text\"]\n",
    "op = call_classification_api_2(text)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def call_classification_api_2(text, mode=\"all\", anonymize=False):\n",
    "    \"\"\"\n",
    "    Call the Pebblo classification API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify\n",
    "        mode (str): Classification mode - \"all\", \"entity\", or \"topic\"\n",
    "        anonymize (bool): Whether to anonymize the results\n",
    "    \n",
    "    Returns:\n",
    "        dict: The classification response\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL.rstrip('/')}/api/v1/classify\"\n",
    "    \n",
    "    # Prepare the request payload without llm_config\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"anonymize\": anonymize,\n",
    "        \"country_list\": [\"US\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #print(\"payload\", payload)\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making API request: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45b24cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishanjain/Desktop/iconect/classifier/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-14 10:32:43.955 - classifier.entity_classifier.core.loader - INFO - Entities config base dir override: None\n",
      "2025-11-14 10:32:43.956 - classifier.entity_classifier.core.loader - INFO - Entities config default base dir: /Users/nishanjain/Desktop/iconect/classifier/myenv/lib/python3.12/site-packages/classifier/entity_classifier)\n",
      "2025-11-14 10:32:44.989 - classifier.entity_classifier.core.loader - INFO - Entities config base dir override: None\n",
      "2025-11-14 10:32:44.990 - classifier.entity_classifier.core.loader - INFO - Entities config default base dir: /Users/nishanjain/Desktop/iconect/classifier/myenv/lib/python3.12/site-packages/classifier/entity_classifier)\n"
     ]
    }
   ],
   "source": [
    "from classifier.entity_classifier.entity_classifier import EntityClassifier\n",
    "\n",
    "entity_classfier = EntityClassifier(countries=[\"US\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627484b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 10:34:21.106 - classifier.entity_classifier.analyzers.base_analyzer - INFO - use_llm is False - skipping LLM detection\n",
      "2025-11-14 10:34:21.108 - classifier.entity_classifier.entity_classifier - INFO - analyzer_results [type: US_SSN, start: 10, end: 21, score: 0.85]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'us-ssn': [{'location': '10_21',\n",
       "    'confidence_score': 0.85,\n",
       "    'entity_value': '321-45-7891',\n",
       "    'start_index': 10,\n",
       "    'end_index': 21}]},\n",
       " 'my ssn is 321-45-7891')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_classfier.entity_classifier_and_anonymizer(\"my ssn is 321-45-7891\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e5a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
